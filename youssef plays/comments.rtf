{\rtf1\ansi\ansicpg1252\cocoartf1343\cocoasubrtf140
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural

\f0\fs24 \cf0 Most variables seem normally distributed with varyious means, but the vars 43-48 seem categorical (42 not really normal, looks like the distributon of y actually, 2 gaussians with different means)\
\
Note that all the gradient descent methods used the eps threshold (machine dependant) in Matlab in the gradient descent function, to allow for a maximum number of iterations. This might make the gradient descent process slow though, so feel free to set another number if needed. We changed it later to 1/10000, so the numbers for the test Errors might be different depending on this parameter when you run the code (the global trends are the same though), since the number of iterations will be different.\
\
Kstest for normality only confirms that first 33 input variables are indeed random. something fishy about the variables 34-42 that SEEM normal (through histogram distribution)  but are not.\
\
tX has rank 49 for regression, so no redundancy in variables there\
tX for classification has rank 36, so 48-35 = 13 of the original data dimensions are transforms or redundant wrt to others -> use PENALIZED logistic regressions and NOT normal logistic regression\
UPDATE: I was wrong, for classification there are 35 variables, + the one vector it SHOULD give rank 36 so the tX is NOT ill conditioned\
\
normalizing the variables 1-33 for regression, plus y, others are categorical\
UPDATE: written a code to check which variables are categorical (basically the ones having less than 15 possible values). For the regression data: variables 35-48 included are categorical (so 33 IS NOT categorical and should be normalized). For the classification: only variables 9, 22, 25.\
\
taking the mean gives an RMSE of 0.99964 (over normalized y)\
\
simply applying LS on the data (with no cleaning) yields RMSE of  0.48244, ridge regression increases it somewhat but stays the same. Compared with the dynamic range of the normalized y_train (-1.5 to 2.94) it\'92s not THAT important. But RMSE is still cut by half, so data IS meaningful.\
\
Tried ignoring the categorical variables for regression, RMSE nearly DOUBLED (went from 0.48244 using LS to 0.84526), so clearly they should NOT be discarded. We might want to modify them, or transform them using dummy variables though.\
\
******\
\
Friday 31st:\
\
Regression:\
By visually inspecting the pearson and spearman correlation graphs, a good strategy to spot well correlated input variables (wrt to the output that is) was to take the intersection of the two methods (we\'92re only considering ). To do that, we tried multiplying the two coefficients (for each variable) and taking the mean. BOTH strategies clearly showed the 18th and 34th variables are best correlated. SO we can limit our variable transform to these two (we only have 1400 data points, so with a dimension 48 we\'92re already lacking data, by introducing spurious transforms we would be adding fuel to the fire and causing overfitting.\
\
Classification:\
\
Repeating the procedure described above, we could find that the 11th and the 24th variables are best correlated with the output, therefore we are going to use feature transforms on those.\
\
Regression :D :D :D :D :P\
\
We got the RMSE down to train error 0.3605 (with estimated test error 0.3805) (from 0.48244) using direct least Squares, simply by adding the squares of the 18th and 34th variables. When we added the squares of ALL the variables, we only got a train error of 0.3405 but this is clearly overfit because the corresponding test error estimate is 0.3765.\
We testing taking the squares of the 18th and the 34th variable (individually) and a curious thing happened: when we removed the 18th variable (squared) the test error increased to to 0.3881, but when we removed the 34th variable (squared) the test error increased much more significantly to 0.4987.\
\
By isolating the 18th and 34th input variables, and producing polynomial transforms of them using the mypoly function, and then measuring their pearson correlation coefficient with the output, we could see that (check graph) the cube and the power 5 of the 18th variable, and powers up to 6 for the 34th variable are correlated with the output. When adding these feature transformations, we could lower the test error to 0.3307, which is quite lower now than the starting test error of 0.48244\
\
Saturday nov 1st:\
\
tried dummy variables for all regression categorical ones. I could only find a significant linear correlation (using spearman) for the 2nd class of the variable 44, 2nd & 4th class of the 47th.\
\
Tried keeping the aforementioned polynomial transforms, and removed ALL categorical variables. Test error actually went down to 0.33043. Adding back the mentioned dummy variables 44 and 47 actually increased test error to 0.33105. Adding only the specifically mentioned (above) classes indicator dummy variables led the tX matrix to become ill conditioned.\
\
I guess even though they are somewhat correlated with the output, these dummy variable spell trouble for the predictor\'85\
\
Removing kevin\'92s variables to ignore actually increased the test Error to 0.33209, something\'92s wrong with the ignoring strategy then\
\
Also tried adding SQRT of the 18th and 34th variable (after making them positive to avoid complex numbers). With the sqrt of the 18th, testErr went up to 0.32137 and with that of the 34th it went up to 0.39349 ! not going to use sqrt\
\
Sunday Nov 2nd:\
\
We decided that we\'92re going to ignore all categorical variables for regression, and also all the un-correlated variables, giving a test error of 0.33173 while including the significant transformations of the 18th and 34th variables discussed before. We decided to opt for a simpler model because of Occam\'92s Razor.\
\
We noticed that we need to change the label of the -1 class of the y_train to 0, to be consistent with the book and to avoid make computations easier later (the probabilities from logistic regression map directly to the classes).\
\
We tied to use simple logistic regression using all the variable, the estimated rmse test error is 0.27099. Also we tried using sqrt on the 11th and 24th variables and it actually improves our test error (0.26779). We can also use squares of the variables instead of the sqrts (after chat there is pearson correlation is as significant as the original feature) and applying this alone the test error decreased to 0.26715. Adding both transformations got us a decrease to a testError of 0.26474. We also tried to remove all categorical variables as we did in regression, but the testError increased to 0.3396, so their information was needed and cannot be ignored.\
\
We then found that (since pearson correlation predicted better correlation of sqrts than squares but then squares were actually better able to decrease testError), it might be a good idea to include 3rd degree transforms of the 11th and 24th variables, so we added them and the testError increased to 0.26656. Thus taking higher orders doesn\'92t seem to help.\
\
We then tried to include a dummy variable transform, but that increased the testError significantly to 0.32625, therefore we\'92re not going to use them.\
\
RidgeRegression: bestParameter =\
 39.0694\
achievedErrorTe =\
    0.3073\
\
Penalized logistic regression: (rmse)\
bestParameter =\
    0.0621\
\
achievedErrorTe =\
    0.2663\
\
Penalized logistic regression: (01 loss)\
bestParameter =\
    0.4642\
\
achievedErrorTe =\
    0.0962\
\
Penalized logistic regression: (log loss)\
bestParameter =\
    3.1623\
\
achievedErrorTe =\
    0.2812}