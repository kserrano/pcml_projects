{\rtf1\ansi\ansicpg1252\cocoartf1343\cocoasubrtf140
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural

\f0\fs24 \cf0 Most variables seem normally distributed with varyious means, but the vars 43-48 seem categorical (42 not really normal, looks like the distributon of y actually, 2 gaussians with different means)\
\
Kstest for normality only confirms that first 33 input variables are indeed random. something fishy about the variables 34-42 that SEEM normal (through histogram distribution)  but are not.\
\
tX has rank 49 for regression, so no redundancy in variables there\
tX for regression has rank 36, so 48-35 = 13 of the original data dimensions are transforms or redundant wrt to others -> use PENALIZED logistic regressions and NOT normal logistic regression\
UPDATE: I was wrong, for classification there are 35 variables, + the one vector it SHOULD give rank 36 so the tX is NOT ill conditioned\
\
normalizing the variables 1-33 for regression, plus y, others are categorical\
UPDATE: written a code to check which variables are categorical (basically the ones having less than 15 possible values). For the regression data: variables 35-48 included are categorical (so 33 IS NOT categorical and should be normalized). For the classification: only variables 9, 22, 25.\
\
taking the mean gives an RMSE of 0.99964 (over normalized y)\
\
simply applying LS on the data (with no cleaning) yields RMSE of  0.48244, ridge regression increases it somewhat but stays the same. Compared with the dynamic range of the normalized y_train (-1.5 to 2.94) it\'92s not THAT important. But RMSE is still cut by half, so data IS meaningful.\
\
Tried ignoring the categorical variables for regression, RMSE nearly DOUBLED (went from 0.48244 using LS to 0.84526), so clearly they should NOT be discarded. We might want to modify them, or transform them using dummy variables though.\
\
******\
\
Friday 31st:\
\
Regression:\
By visually inspecting the pearson and spearman correlation graphs, a good strategy to spot well correlated input variables (wrt to the output that is) was to take the intersection of the two methods (we\'92re only considering ). To do that, we tried multiplying the two coefficients (for each variable) and taking the mean. BOTH strategies clearly showed the 18th and 34th variables are best correlated. SO we can limit our variable transform to these two (we only have 1400 data points, so with a dimension 48 we\'92re already lacking data, by introducing spurious transforms we would be adding fuel to the fire and causing overfitting.\
\
Classification:\
\
Repeating the procedure described above, we could find that the 11th and the 24th variables are best correlated with the output, therefore we are going to use feature transforms on those.\
\
Regression :D :D :D :D :P\
\
We got the RMSE down to train error 0.3605 (with estimated test error 0.3805) (from 0.48244) using direct least Squares, simply by adding the squares of the 18th and 34th variables. When we added the squares of ALL the variables, we only got a train error of 0.3405 but this is clearly overfit because the corresponding test error estimate is 0.3765.\
We testing taking the squares of the 18th and the 34th variable (individually) and a curious thing happened: when we removed the 18th variable (squared) the test error increased to to 0.3881, but when we removed the 34th variable (squared) the test error increased much more significantly to 0.4987.\
\
By isolating the 18th and 34th input variables, and producing polynomial transforms of them using the mypoly function, and then measuring their pearson correlation coefficient with the output, we could see that (check graph) the cube and the power 5 of the 18th variable, and powers up to 6 for the 34th variable are correlated with the output. When adding these feature transformations, we could lower the test error to 0.3307, which is quite lower now than the starting test error of 0.48244\
\
-  }