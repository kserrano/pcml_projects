\documentclass{article} % For LaTeX2e
% We will use NIPS submission format
\usepackage{nips13submit_e,times}
% for hyperlinks
\usepackage{hyperref}
\usepackage{url}
% For figures
\usepackage{graphicx} 
\usepackage{subfigure} 
% math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsopn}
\usepackage{ifthen}
\usepackage{natbib}

\title{Project-I by Group MexicoCity}

\author{
Kevin Serrano\\EPFL\\
\texttt{kevin.serrano@epfl.ch} \And Youssef El Baba\\EPFL\\
\texttt{youssef.baba@epfl.ch} \And Alexandre Helfre\\EPFL\\
\texttt{alexandre.helfre@epfl.ch}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\nipsfinalcopy 

\begin{document}

\maketitle

\begin{abstract}
In this report, we discuss our implementation and findings for the project-I. \textcolor{red}{Todo at the end}


\end{abstract}
\section{Five functions}
\begin{itemize}
\item \textit{leastSquaresGD(y,tX,alpha)} : Least squares using gradient descent, alpha is the step-size
\item \textit{leastSquares(y,tX)} : Least squares using normal equations
\item \textit{ridgeRegression(y,tX,lambda)} : Ridge regression using normal equations, lambda is the regularization coefficient
\item \textit{logisticRegression(y,tX,alpha)} : Logistic regression using gradient descent or Newton's method
\item \textit{penLogisticRegression(y,tX,alpha,lambda)} : Penalized logistic regression 

\end{itemize}
These functions are different machine learning methods.

\textcolor{red}{Formula? Not even sure this section is useful}

\section{Data observation}
We have two data set. One for regression and one for classification. \begin{description}
\item[Regression] Consists of output variables $\mathbf{y}$ and input variables $\mathbf{X}$. The number of examples is $N = 1400$ and each input $\mathbf{x_n}$ has dimensionality $D = 48$. The first $34$ are real valued and the last $14$ are categorical, included $5$ that are binaries.
\item[Classification] Consists of output variables $\mathbf{y}$ and input variables $\mathbf{X}$. The number of examples is $N = 1500$ and each input $\mathbf{x_n}$ has dimensionality $D = 35$. \textcolor{red}{check hist}

\end{description}


\section{Data visualization and cleaning}
\textcolor{red}{Histogram, correlation, applied methods}\\
Most variables normally distributed with various means. The first 34 input variables are Gaussian and the last 14 are categorical. Given that, we normalized and centred the first 34 variables and let the others untouched. Plotting the histogram showed us that the data are compact and there is no outlier.

\section{Best method : Ridge Regression}
\textcolor{red}{explain what is the best method and why for our dataset, add some figures and results}
\section{Feature transformations}
\textcolor{red}{Different transformation (myPoly, sqrt, etc)}
\section{Summary}
\textcolor{red}{summarize in a few lines and write down all the final results}


\subsubsection*{Acknowledgments}


\subsubsection*{References}

\end{document}
